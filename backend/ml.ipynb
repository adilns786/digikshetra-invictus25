{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aadil\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aadil\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aadil\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/8.1 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.8/8.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/8.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.1 seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadil\\AppData\\Local\\Temp\\ipykernel_15572\\8928252.py:139: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: P1001, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateParseError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 355\u001b[39m\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, probabilities\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Load and prepare the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     df = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproperty_transactions.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Exploratory Data Analysis\u001b[39;00m\n\u001b[32m     19\u001b[39m     print_data_summary(df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m    136\u001b[39m     df[col] = df[col].map({\u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Convert timestamp to datetime\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Extract coordinates into separate latitude and longitude columns\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mcoordinates\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1065\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1066\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    447\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aadil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2398\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2395\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2396\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2398\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2408\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2409\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:414\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtslib.pyx:553\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mconversion.pyx:641\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsing.pyx:336\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsing.pyx:666\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDateParseError\u001b[39m: Unknown datetime string format, unable to parse: P1001, at position 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def main():\n",
    "    # Load and prepare the data\n",
    "    df = load_data('property_transactions.csv')\n",
    "    \n",
    "    # Exploratory Data Analysis\n",
    "    print_data_summary(df)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_target(df)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(X, y)\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(model, 'real_estate_fraud_model.pkl')\n",
    "    \n",
    "    print(\"Model training complete and saved to 'real_estate_fraud_model.pkl'\")\n",
    "    \n",
    "    # Function to make predictions on new data\n",
    "    if len(df) > 0:\n",
    "        print(\"\\nExample prediction:\")\n",
    "        sample = df.iloc[0:1].copy()\n",
    "        print(f\"Transaction ID: {sample['transaction_id'].values[0]}\")\n",
    "        prediction = predict(model, sample)\n",
    "        print(f\"Fraud prediction: {prediction[0]}\")\n",
    "        print(f\"Fraud probability: {prediction[1][0][1]:.4f}\")\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and parse CSV data\"\"\"\n",
    "    try:\n",
    "        # Handle the custom CSV format - first trying the normal way\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except:\n",
    "            # If that fails, try to parse the data manually\n",
    "            print(\"Standard CSV parsing failed. Trying manual parsing...\")\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Get headers from first line\n",
    "            headers = lines[0].strip().split(',')\n",
    "            \n",
    "            # Parse data rows\n",
    "            data = []\n",
    "            for line in lines[1:]:\n",
    "                # Handle potential commas within quoted strings\n",
    "                values = []\n",
    "                in_quotes = False\n",
    "                current_value = \"\"\n",
    "                \n",
    "                for char in line:\n",
    "                    if char == '\"' and not in_quotes:\n",
    "                        in_quotes = True\n",
    "                    elif char == '\"' and in_quotes:\n",
    "                        in_quotes = False\n",
    "                    elif char == ',' and not in_quotes:\n",
    "                        values.append(current_value)\n",
    "                        current_value = \"\"\n",
    "                    else:\n",
    "                        current_value += char\n",
    "                \n",
    "                # Add the last value\n",
    "                if current_value:\n",
    "                    values.append(current_value)\n",
    "                \n",
    "                # Make sure we have the right number of values\n",
    "                if len(values) == len(headers):\n",
    "                    data.append(values)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping malformed line with {len(values)} values instead of {len(headers)}\")\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        print(\"Creating sample data instead.\")\n",
    "        \n",
    "        # Create a sample dataframe from the provided example\n",
    "        sample_data = \"\"\"transaction_id,timestamp,property_id,seller_id,buyer_id,property_type,area_sqft,listed_price,transaction_price,price_per_sqft,location,coordinates,title_age_days,amenities,nearby_landmarks,ownership_changes_count,days_on_market,has_extract7_12,has_mutation_certificate,has_property_tax_receipt,has_sale_deed,legal_compliance_complete,price_change_percent,buyer_seller_relation,agent_involved,transaction_speed_days,multiple_transaction_30days,seller_previous_fraud,is_fraud\n",
    "TR001,2024-12-15 08:30:00,P1001,S101,B201,residential,1200,150000,148000,123.33,Downtown,40.7128,-74.0060,450,pool/gym/parking,park/school/mall,1,45,true,true,true,true,true,-1.33,unrelated,true,15,false,false,false\"\"\"\n",
    "        \n",
    "        # Generate more sample data to have enough for training\n",
    "        import random\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        lines = [sample_data.strip()]\n",
    "        base_date = datetime.strptime(\"2024-12-15 08:30:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        for i in range(2, 500):\n",
    "            # Generate a mix of fraudulent and non-fraudulent transactions (20% fraud)\n",
    "            is_fraud = random.random() < 0.2\n",
    "            \n",
    "            tr_id = f\"TR{i:03d}\"\n",
    "            timestamp = (base_date - timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            property_id = f\"P{1000+i}\"\n",
    "            seller_id = f\"S{100+random.randint(1, 50)}\"\n",
    "            buyer_id = f\"B{200+random.randint(1, 50)}\"\n",
    "            \n",
    "            # Create realistic patterns for fraudulent transactions\n",
    "            if is_fraud:\n",
    "                # Fraud patterns\n",
    "                property_type = random.choice([\"residential\", \"commercial\", \"land\"])\n",
    "                area_sqft = random.randint(800, 5000)\n",
    "                listed_price = random.randint(100000, 500000)\n",
    "                # Significant price difference in fraudulent transactions\n",
    "                transaction_price = listed_price * (1 + random.uniform(0.15, 0.3) * random.choice([-1, 1]))\n",
    "                price_per_sqft = transaction_price / area_sqft\n",
    "                location = random.choice([\"Remote Area\", \"Industrial Zone\", \"Outskirts\", \"Downtown\"])\n",
    "                coordinates = f\"{round(random.uniform(40, 41), 4)} {round(random.uniform(-74, -73), 4)}\"\n",
    "                title_age_days = random.randint(5, 100) if random.random() < 0.7 else random.randint(450, 900)\n",
    "                amenities = random.choice([\"none\", \"parking\", \"basic\"])\n",
    "                nearby_landmarks = random.choice([\"none\", \"factory\", \"highway\"])\n",
    "                ownership_changes_count = random.randint(2, 5)\n",
    "                days_on_market = random.randint(1, 15)\n",
    "                has_extract7_12 = str(random.random() > 0.4).lower()\n",
    "                has_mutation_certificate = str(random.random() > 0.4).lower()\n",
    "                has_property_tax_receipt = str(random.random() > 0.3).lower()\n",
    "                has_sale_deed = str(random.random() > 0.2).lower()\n",
    "                legal_compliance_complete = \"false\"\n",
    "                price_change_percent = round(random.uniform(-15, 15), 2)\n",
    "                buyer_seller_relation = random.choice([\"family\", \"business_associate\", \"unrelated\"])\n",
    "                agent_involved = str(random.random() > 0.3).lower()\n",
    "                transaction_speed_days = random.randint(1, 7)\n",
    "                multiple_transaction_30days = str(random.random() > 0.6).lower()\n",
    "                seller_previous_fraud = str(random.random() > 0.7).lower()\n",
    "            else:\n",
    "                # Non-fraud patterns\n",
    "                property_type = random.choice([\"residential\", \"commercial\", \"land\"])\n",
    "                area_sqft = random.randint(800, 5000)\n",
    "                listed_price = random.randint(100000, 500000)\n",
    "                # Smaller price difference in legitimate transactions\n",
    "                transaction_price = listed_price * (1 + random.uniform(-0.05, 0.05))\n",
    "                price_per_sqft = transaction_price / area_sqft\n",
    "                location = random.choice([\"Downtown\", \"Suburbs\", \"City Center\", \"Residential Area\"])\n",
    "                coordinates = f\"{round(random.uniform(40, 41), 4)} {round(random.uniform(-74, -73), 4)}\"\n",
    "                title_age_days = random.randint(300, 900)\n",
    "                amenities = random.choice([\"pool/gym/parking\", \"gym/parking\", \"parking\", \"pool/parking\"])\n",
    "                nearby_landmarks = random.choice([\"park/school/mall\", \"school/hospital\", \"mall/park\", \"school/market\"])\n",
    "                ownership_changes_count = random.randint(0, 2)\n",
    "                days_on_market = random.randint(30, 120)\n",
    "                has_extract7_12 = \"true\"\n",
    "                has_mutation_certificate = \"true\"\n",
    "                has_property_tax_receipt = \"true\"\n",
    "                has_sale_deed = \"true\"\n",
    "                legal_compliance_complete = \"true\"\n",
    "                price_change_percent = round(random.uniform(-5, 5), 2)\n",
    "                buyer_seller_relation = \"unrelated\"\n",
    "                agent_involved = \"true\"\n",
    "                transaction_speed_days = random.randint(15, 60)\n",
    "                multiple_transaction_30days = \"false\"\n",
    "                seller_previous_fraud = \"false\"\n",
    "            \n",
    "            line = f\"{tr_id},{timestamp},{property_id},{seller_id},{buyer_id},{property_type},{area_sqft},{listed_price:.0f},{transaction_price:.0f},{price_per_sqft:.2f},{location},{coordinates},{title_age_days},{amenities},{nearby_landmarks},{ownership_changes_count},{days_on_market},{has_extract7_12},{has_mutation_certificate},{has_property_tax_receipt},{has_sale_deed},{legal_compliance_complete},{price_change_percent},{buyer_seller_relation},{agent_involved},{transaction_speed_days},{multiple_transaction_30days},{seller_previous_fraud},{str(is_fraud).lower()}\"\n",
    "            lines.append(line)\n",
    "        \n",
    "        sample_data = \"\\n\".join(lines)\n",
    "        df = pd.read_csv(pd.StringIO(sample_data))\n",
    "    \n",
    "    # Clean data and convert types\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess the dataframe\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['area_sqft', 'listed_price', 'transaction_price', 'price_per_sqft', \n",
    "                   'title_age_days', 'ownership_changes_count', 'days_on_market',\n",
    "                   'price_change_percent', 'transaction_speed_days']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert boolean string values to actual booleans\n",
    "    bool_columns = [\n",
    "        'has_extract7_12', 'has_mutation_certificate', 'has_property_tax_receipt',\n",
    "        'has_sale_deed', 'legal_compliance_complete', 'agent_involved',\n",
    "        'multiple_transaction_30days', 'seller_previous_fraud', 'is_fraud'\n",
    "    ]\n",
    "    \n",
    "    for col in bool_columns:\n",
    "        if col in df.columns:\n",
    "            # Handle different forms of boolean values\n",
    "            df[col] = df[col].astype(str).str.lower()\n",
    "            df[col] = df[col].map({'true': True, 'false': False, 't': True, 'f': False, \n",
    "                                   'yes': True, 'no': False, '1': True, '0': False,\n",
    "                                   'y': True, 'n': False})\n",
    "    \n",
    "    # Convert timestamp to datetime - handle different formats\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        \n",
    "        # Extract time-based features if conversion was successful\n",
    "        if not df['timestamp'].isna().all():\n",
    "            df['month'] = df['timestamp'].dt.month\n",
    "            df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "            df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Handle coordinates - could be in various formats\n",
    "    if 'coordinates' in df.columns:\n",
    "        def extract_coordinates(coord_str):\n",
    "            if pd.isna(coord_str):\n",
    "                return np.nan, np.nan\n",
    "            \n",
    "            # Try to extract numbers from the string\n",
    "            numbers = re.findall(r'-?\\d+\\.?\\d*', str(coord_str))\n",
    "            if len(numbers) >= 2:\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Extract coordinates\n",
    "        df['latitude'], df['longitude'] = zip(*df['coordinates'].apply(extract_coordinates))\n",
    "    \n",
    "    # Process text list fields - handle different delimiters\n",
    "    if 'amenities' in df.columns:\n",
    "        df['amenities_count'] = df['amenities'].astype(str).apply(\n",
    "            lambda x: len(re.split(r'[,/|;]', x)) if not pd.isna(x) and x.lower() != 'none' else 0)\n",
    "    \n",
    "    if 'nearby_landmarks' in df.columns:\n",
    "        df['landmarks_count'] = df['nearby_landmarks'].astype(str).apply(\n",
    "            lambda x: len(re.split(r'[,/|;]', x)) if not pd.isna(x) and x.lower() != 'none' else 0)\n",
    "    \n",
    "    # Calculate price discrepancy\n",
    "    if all(col in df.columns for col in ['listed_price', 'transaction_price']):\n",
    "        df['price_discrepancy'] = (df['transaction_price'] - df['listed_price']).abs() / df['listed_price'] * 100\n",
    "        df['price_ratio'] = df['transaction_price'] / df['listed_price']\n",
    "    \n",
    "    # Handle missing values in important columns\n",
    "    if 'property_type' in df.columns and df['property_type'].isna().any():\n",
    "        df['property_type'] = df['property_type'].fillna('unknown')\n",
    "    \n",
    "    if 'location' in df.columns and df['location'].isna().any():\n",
    "        df['location'] = df['location'].fillna('unknown')\n",
    "    \n",
    "    if 'buyer_seller_relation' in df.columns and df['buyer_seller_relation'].isna().any():\n",
    "        df['buyer_seller_relation'] = df['buyer_seller_relation'].fillna('unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_data_summary(df):\n",
    "    \"\"\"Print summary of the dataset\"\"\"\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    if 'is_fraud' in df.columns:\n",
    "        print(f\"Number of fraudulent transactions: {df['is_fraud'].sum()} ({df['is_fraud'].mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Print correlation with fraud if it exists\n",
    "    if 'is_fraud' in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            fraud_corr = df[numeric_cols].corrwith(df['is_fraud']).sort_values(ascending=False)\n",
    "            print(\"\\nTop correlations with fraud:\")\n",
    "            print(fraud_corr.head(10))\n",
    "            \n",
    "            # Create feature importance plot if we have enough data\n",
    "            if len(df) >= 100:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                fraud_features = abs(fraud_corr).sort_values(ascending=False).head(10)\n",
    "                sns.barplot(x=fraud_features.values, y=fraud_features.index)\n",
    "                plt.title('Feature Correlation with Fraud')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('fraud_correlation.png')\n",
    "                print(\"\\nFeature correlation plot saved as 'fraud_correlation.png'\")\n",
    "\n",
    "def prepare_features_target(df):\n",
    "    \"\"\"Prepare features and target variable\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop columns that shouldn't be used for modeling\n",
    "    drop_cols = ['transaction_id', 'timestamp', 'property_id', 'seller_id', 'buyer_id', \n",
    "                 'coordinates', 'amenities', 'nearby_landmarks']\n",
    "    \n",
    "    if 'is_fraud' in df.columns:\n",
    "        drop_cols.append('is_fraud')\n",
    "    \n",
    "    # Ensure all columns exist before attempting to drop\n",
    "    drop_cols = [col for col in drop_cols if col in df.columns]\n",
    "    \n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Handle target variable\n",
    "    if 'is_fraud' in df.columns:\n",
    "        y = df['is_fraud']\n",
    "    else:\n",
    "        # For prediction on new data without target\n",
    "        y = None\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X, y):\n",
    "    \"\"\"Train a machine learning model\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Define feature types\n",
    "    categorical_features = [col for col in X.columns if col in [\n",
    "        'property_type', 'location', 'buyer_seller_relation'\n",
    "    ]]\n",
    "    \n",
    "    boolean_features = [col for col in X.columns if col in [\n",
    "        'has_extract7_12', 'has_mutation_certificate', 'has_property_tax_receipt',\n",
    "        'has_sale_deed', 'legal_compliance_complete', 'agent_involved',\n",
    "        'multiple_transaction_30days', 'seller_previous_fraud'\n",
    "    ]]\n",
    "    \n",
    "    # All remaining columns are numeric\n",
    "    numeric_features = [col for col in X.columns \n",
    "                        if col not in categorical_features + boolean_features]\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            \n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_features if categorical_features else []),\n",
    "            \n",
    "            ('bool', 'passthrough', boolean_features if boolean_features else [])\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # Create and train the model pipeline\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Hyperparameter tuning - use smaller grid for faster execution\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100],\n",
    "        'classifier__max_depth': [None, 15],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    }\n",
    "    \n",
    "    # Use an even smaller parameter grid if we have a very small dataset\n",
    "    if len(X) < 100:\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100],\n",
    "            'classifier__max_depth': [None]\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=min(5, len(y_train) // 10 or 2), \n",
    "        scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)\n",
    "    \n",
    "    print(\"\\nBest model parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba[:, 1]):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(best_model['classifier'], 'feature_importances_'):\n",
    "        try:\n",
    "            # Try to get feature names from pipeline\n",
    "            features_out = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "            importances = best_model['classifier'].feature_importances_\n",
    "            \n",
    "            if len(importances) == len(features_out):\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                \n",
    "                print(\"\\nTop 10 important features:\")\n",
    "                for i, idx in enumerate(indices[:min(10, len(indices))]):\n",
    "                    print(f\"{i+1}. {features_out[idx]} ({importances[idx]:.4f})\")\n",
    "                \n",
    "                # Plot feature importances\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                top_indices = indices[:min(10, len(indices))]\n",
    "                sns.barplot(y=[features_out[i] for i in top_indices], \n",
    "                          x=importances[top_indices], \n",
    "                          orient='h')\n",
    "                plt.title('Feature Importances')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('feature_importance.png')\n",
    "                print(\"\\nFeature importance plot saved as 'feature_importance.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot feature importances: {e}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"Save the trained model to disk\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        print(f\"Model successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"Load a trained model from disk\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict(model, data):\n",
    "    \"\"\"Make predictions using the trained model\"\"\"\n",
    "    # Ensure data is properly prepared\n",
    "    data_clean = clean_data(data)\n",
    "    X = prepare_features_target(data_clean)[0]\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X)\n",
    "    probabilities = model.predict_proba(X)\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
